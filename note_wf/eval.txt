Evaluation proceeds in four steps:

Selection - Predictions not within a video's scoring intervals are dropped.
Assignment - Predicted events are matched with ground-truth events.
Scoring -    Each group of predictions is scored against  its corresponding group of ground-truth events
                via Average Precision.
Reduction - The multiple AP scores are ¿averaged¿ to produce a single overall score.      


Selection
    With each video
    there is a defined set of scoring intervals
    giving the intervals of time over which
    >= 0 ground-truth events 
    might be annotated in that  video.
    A prediction is only evaluated if
    it falls within a scoring interval.
    These scoring intervals
    were chosen to improve the fairness of evaluation by
    ,for instance,  ¿ignoring¿ edge-cases or  ambiguous events.   

    For reference,
    we provide the scoring intervals for the ¿training¿ data.
    however,
    The scoring intervals for the test data
        will not be available to your model during evaluation   

Assignment
    For each set of predictions and ground-truths 
        within the same `event x tolerance x video_id` group, (笛卡尔积?)
            we match each ground-truth to
            the highest-confidence unmatched prediction
                occurring within the allowed tolerance.    

    Some ground-truths may not be matched to a prediction,  (FN?)
    some predictions may not be matched to a ground-truth.  (FP?)

  
Scoring 
    Collecting the events within each video_id,
    we compute an Average Precision score for each event x tolerance group.
    The average precision score is the area under the precision-recall curve generated by
    decreasing confidence score thresholds over the predictions.
    In this calculation,
    matched predictions over the threshold are scored as TP and
    unmatched predictions as FP.
    Unmatched ground-truths are scored as FN.
   
Average 
    The final score
    is the average of the above AP scores,
        first averaged over tolerance,
        then over event.
     


    
    https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout/discussion/341548
    The public Python metric is just meant to be a convenience function to help people develop solutions.
    Our actual deployed metric has a lot more checks to prevent exploitations like this.
    I don't believe this trick would work with the actual competition metric.
          

About tolerance
    The goal of the competition is to develop algorithms that predict with a certain tolerance.
    Why complicate the metric with averages over multiple tolerances?    

    Averaging over a range of tolerances gives us
        a kind of linear weighting   to tolerance errors,
        instead of a simple right/wrong.
        It just lets us score performance among models with
        a bit more distinctiveness.
    Averaging  is  common in detection metrics.     

    In order to understand the contents of the evaluation index, see:
        https://www.kaggle.com/code/hidebu/exploration-of-competition-metric-dfl-ap-jp-en 

